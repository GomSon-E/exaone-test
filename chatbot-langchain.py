import os
import re
import json
import torch
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

app = FastAPI()

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
pdf_paths = [
    "ì •ì±…ê³µì•½ì§‘.pdf",
    "ì§€ì—­ê³µì•½.pdf"
]

# ì „ì—­ ë³€ìˆ˜ë¡œ LangChain êµ¬ì„±ìš”ì†Œ ì„ ì–¸
retriever = None
llm = None
rag_chain = None

# 1. ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì¿¼ë¦¬ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
query_transformation_prompt = PromptTemplate.from_template(
    """ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë¬¸ì„œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ìµœì ì˜ ê²€ìƒ‰ì–´ë§Œ ì¶œë ¥. ì„¤ëª…ì€ ë¶ˆí•„ìš”.
    
    ì›ë˜ ì§ˆë¬¸: {question}

    ê²€ìƒ‰ì–´ : 
    """
)

# 2. ì‘ë‹µ ìƒì„± í”„ë¡¬í”„íŠ¸
answer_generation_prompt = PromptTemplate.from_template(
    """1. ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ì±…/ê³µì•½ ì œëª© ì¶”ì¶œ
    2. ê´€ë ¨ ì •ì±…ì´ ì—†ë‹¤ë©´ "ê´€ë ¨ ì •ì±… ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µ
    3. ì¶”ì¶œëœ ì •ë³´ë¥¼ ë°”íƒ• 2~3ê°€ì§€ ê³µì•½ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    
    ì§ˆë¬¸: {question}
    
    ë¬¸ì„œ ë‚´ìš©:
    {context}
    
    JSON í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì•„ì•¼ í•¨ : 
    ```json
    {{
        "ê³µì•½": [ "ì²«ë²ˆì§¸ ê³µì•½ ì œëª© ë˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ", "ë‘ë²ˆì§¸ ê³µì•½ ì œëª©", ... ],
    }}
    ```

    JSON ë‹µë³€ : 
    """
)

# ì „ì²´ ë©€í‹°ëª¨ë‹¬ RAG ì²´ì¸ êµ¬ì„±
def create_multimodal_rag_chain(retriever, llm):
    # 1. ì¿¼ë¦¬ ë³€í™˜ ì²´ì¸
    query_transformer_chain = (
        {"question": RunnablePassthrough()}
        | query_transformation_prompt
        | llm
        | StrOutputParser()
    )
    
    # ê²€ìƒ‰ ì²´ì¸
    def retrieve_documents(input_dict):
        original_question = input_dict["original_question"]
        optimized_query = input_dict["optimized_query"]
        
        # ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        retrieved_docs = retriever.invoke(optimized_query)
        
        return {
            "question": original_question,
            "context": "\n\n".join([doc.page_content for doc in retrieved_docs])
        }
    
    # ì‘ë‹µ ìƒì„± ì²´ì¸
    answer_chain = (
        answer_generation_prompt
        | llm
        | StrOutputParser()
    )
    
    # í›„ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ - JSON ì‘ë‹µ ì²˜ë¦¬
    def format_answer(answer):
        print(answer)
        json_pattern = r'JSON ë‹µë³€ :\s*```(?:json)?\s*([\s\S]*?)```'
        json_match = re.search(json_pattern, answer)
        
        if json_match:
            # JSON ì½”ë“œ ë¸”ë¡ ë‚´ìš© ì¶”ì¶œ
            json_str = json_match.group(1).strip()
            return json_str
        
        return None
    
    # ì „ì²´ ë©€í‹°ëª¨ë‹¬ ì²´ì¸ êµ¬ì„±
    multimodal_chain = (
        # 1ë‹¨ê³„: ì›ë³¸ ì§ˆë¬¸ ì €ì¥ ë° ì¿¼ë¦¬ ìµœì í™”
        RunnablePassthrough().with_config(run_name="Original Question")
        | {"original_question": lambda x: x, "optimized_query": query_transformer_chain}
        
        # 2ë‹¨ê³„: ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰
        | RunnableLambda(retrieve_documents).with_config(run_name="Document Retrieval")
        
        # 3ë‹¨ê³„: í†µí•©ëœ ë¬¸ì„œ ë¶„ì„ ë° ìµœì¢… ì‘ë‹µ ìƒì„± (2ë²ˆê³¼ 3ë²ˆ ë‹¨ê³„ í†µí•©)
        | answer_chain
        
        # 4ë‹¨ê³„: ì‘ë‹µ í›„ì²˜ë¦¬
        | RunnableLambda(format_answer)
    )
    
    return multimodal_chain

def init_rag_system():
    """LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global retriever, llm, rag_chain

    print("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1. PDF ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = []
    for pdf_path in pdf_paths:
        if os.path.exists(pdf_path):
            loader = PyPDFLoader(pdf_path)
            documents.extend(loader.load())
        else:
            print(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")

    if not documents:
        print("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    print(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # 3. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")

    # ìƒ˜í”Œ ë¬¸ì„œ í™•ì¸ (ë””ë²„ê¹…ìš©)
    for i, doc_id in enumerate(list(vectorstore.docstore._dict.keys())[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
        print(f"ë¬¸ì„œ ID {doc_id}ì˜ ë‚´ìš©:")
        print(vectorstore.docstore._dict[doc_id])
        print("-" * 50)
        if i >= 2:  # ìµœëŒ€ 3ê°œë§Œ ì¶œë ¥
            break

    # 5. ê²€ìƒ‰ê¸°(Retriever) ì„¤ì • - ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì§ì ‘ ìƒì„±
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 2}
    )

    # 6. EXAONE ëª¨ë¸ ë¡œë“œ ë° LangChain LLM ë˜í¼ ì„¤ì •
    model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
    print(f"{model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ì–‘ìí™” ì„¤ì •ì„ BitsAndBytesConfigë¡œ êµ¬ì„±
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    # ëª¨ë¸ ë¡œë“œ - ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )

    # ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©
    optimize_performance(model)

    # ì§ì ‘ Hugging Face íŒŒì´í”„ë¼ì¸ ìƒì„± í›„ LangChain ë˜í¼ ì ìš©
    from transformers import pipeline

    # íŠ¸ëœìŠ¤í¬ë¨¸ íŒŒì´í”„ë¼ì¸ ìƒì„±
    hf_pipeline = pipeline(
        task="text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=100,
        # do_sample=True,
        # temperature=0.3,
        device_map="auto",
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id
    )

    # LangChain HuggingFacePipeline ë˜í¼ ìƒì„±
    llm = HuggingFacePipeline(pipeline=hf_pipeline)

    print("EXAONE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    rag_chain = create_multimodal_rag_chain(retriever, llm)

    print("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    return True

def optimize_performance(model):
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©"""
    # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
        torch.cuda.set_device(torch.cuda.current_device())

    # ëª¨ë¸ ìµœì í™”
    if hasattr(model, 'config'):
        if hasattr(model.config, 'use_cache'):
            model.config.use_cache = True
        if hasattr(model.config, 'gradient_checkpointing'):
            model.config.gradient_checkpointing = False

    print("ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê¸°ë³¸ ê²½ë¡œ í…ŒìŠ¤íŠ¸ìš©
@app.get('/')
def hello_world():
    return 'ì¹´ì¹´ì˜¤í†¡ ì±—ë´‡ LLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤!'

# ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì—”ë“œí¬ì¸íŠ¸
@app.post('/api/chat')
async def kakao_skill(request: Request):
    global rag_chain
    
    # ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ìš”ì²­ íŒŒì‹±
    req = await request.json()

    # ì‚¬ìš©ì ë°œí™”(query) ì¶”ì¶œ
    user_query = req['userRequest']['utterance']

    # ë©€í‹°ëª¨ë‹¬ LangChain ì²´ì¸ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
    answer = rag_chain.invoke(user_query)

    # JSON ë¬¸ìì—´ íŒŒì‹±
    parsed_json = json.loads(answer)

    # ê³µì•½ ëª©ë¡ ì¶”ì¶œ
    policies = parsed_json.get("ê³µì•½", [])

    # ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    if policies:
        # ê³µì•½ì´ ìˆëŠ” ê²½ìš°
        description = ""
        for idx, policy in enumerate(policies, 1):
            description += f"{idx}. {policy}\n"
    else:
        # ê³µì•½ì´ ì—†ëŠ” ê²½ìš°
        description = parsed_json.get("ë©”ì‹œì§€", "ìš”ì²­í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    res = {
        "version": "2.0",
        "template": {
            "outputs": [
                {
                    "textCard": {
                        "title": f'ğŸ¤– {user_query} ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.',
                        "description": description,
                        "buttons": [
                            {
                                "action": "webLink",
                                "label": "ğŸŒ ì§€ì—­ê³µì•½ ì‚´í´ë³´ê¸°",
                                "webLinkUrl": "https://www.naver.com/"
                            },
                            {
                                "action": "webLink",
                                "label": "ğŸ“š ì •ì±…ê³µì•½ì§‘ ì‚´í´ë³´ê¸°",
                                "webLinkUrl": "https://www.naver.com/"
                            }
                        ]
                    }
                }
            ]
        }
    }

    print(res)

    return res

# ì„œë²„ ì´ˆê¸°í™” ë° ì‹¤í–‰ì„ ìœ„í•œ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    # ëª¨ë¸ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    init_success = init_rag_system()

    if not init_success:
        print("ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        import sys
        sys.exit(1)
    else:
        print("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ! ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    uvicorn.run("chatbot-langchain:app", host="0.0.0.0", port=5000)
                                                                                      
