import os
import re
import torch
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

app = FastAPI()

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
pdf_paths = [
    "ì •ì±…ê³µì•½ì§‘.pdf",
    "ì§€ì—­ê³µì•½.pdf"
]

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ ë²¡í„° ìŠ¤í† ì–´ ì„ ì–¸
model = None
tokenizer = None
vectorstore = None

def init_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global model, tokenizer, vectorstore

    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1. PDF ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = []
    for pdf_path in pdf_paths:
        if os.path.exists(pdf_path):
            loader = PyPDFLoader(pdf_path)
            documents.extend(loader.load())
        else:
            print(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")

    if not documents:
        print("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    print(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # 3. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")

    for doc_id in list(vectorstore.docstore._dict.keys()):
        print(f"ë¬¸ì„œ ID {doc_id}ì˜ ë‚´ìš©:")
        print(vectorstore.docstore._dict[doc_id])
        print("-" * 50)

    # 5. EXAONE ëª¨ë¸ ë¡œë“œ
    model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
    print(f"{model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ì–‘ìí™” ì„¤ì •ì„ BitsAndBytesConfigë¡œ êµ¬ì„±
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    # ëª¨ë¸ ë¡œë“œ - ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("EXAONE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©
    optimize_performance()

    return True

def optimize_performance():
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©"""
    global model

    # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
        torch.cuda.set_device(torch.cuda.current_device())

    # ëª¨ë¸ ìµœì í™”
    if hasattr(model, 'config'):
        if hasattr(model.config, 'use_cache'):
            model.config.use_cache = True
        if hasattr(model.config, 'gradient_checkpointing'):
            model.config.gradient_checkpointing = False

    print("ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

def retrieve_context(query, k):
    """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    global vectorstore
    relevant_docs = vectorstore.similarity_search(query, k=k)

    context = "\n\n".join([doc.page_content for doc in relevant_docs])
    return context

def generate_answer(prompt, max_new_tokens, temperature):
    """í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    global model, tokenizer

    # ì…ë ¥ ì¸ì½”ë”© - attention_mask ëª…ì‹œì  ì„¤ì •
    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)

    # íš¨ìœ¨ì ì¸ ìƒì„± ì„¤ì • - ìˆ˜ì •ëœ ë¶€ë¶„
    generation_config = {
        "max_new_tokens": max_new_tokens,
        "do_sample": True,
        "temperature": temperature,
        "num_beams": 1,
        "pad_token_id": tokenizer.eos_token_id,
    }

    # í† í° ìƒì„±
    with torch.no_grad():
        output = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,  # attention_mask ëª…ì‹œì  ì „ë‹¬
            **generation_config
        )

    # ì‘ë‹µ ë””ì½”ë”©
    response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response.strip()

def answer_with_rag(query, k=5, max_tokens=100, temperature=0.5):
    """RAGë¡œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    context = retrieve_context(query, k=k)

    # ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f""" ì œê³µí•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
                ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
                ì‚¬ìš©ì ìš”ì²­ í‚¤ì›Œë“œë¥¼ ë‹µë³€ì— ë˜í’€ì´ í•˜ì§€ ë§ˆì‹œì˜¤.
                ### ì°¸ê³  ì •ë³´:{context} ### ì‚¬ìš©ì ì§ˆë¬¸ : {query}ì™€ ê´€ë ¨ëœ 2~3ê°€ì§€ ê³µì•½ë§Œ 100ì ì´í•˜ë¡œ ë‹µë³€í•´ì¤˜.  ###ë‹µë³€:"""

    # ì‘ë‹µ ìƒì„± - í† í° ìˆ˜ì™€ temperature ìµœì í™”
    answer = generate_answer(prompt, max_new_tokens=max_tokens, temperature=temperature)
    answer = re.sub(r'\*\*(.*?)\*\*', r'\1', answer)
    return answer

# ê¸°ë³¸ ê²½ë¡œ í…ŒìŠ¤íŠ¸ìš©
@app.get('/')
def hello_world():
    return 'ì¹´ì¹´ì˜¤í†¡ ì±—ë´‡ LLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤!'

# ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì—”ë“œí¬ì¸íŠ¸
@app.post('/api/chat')
async def kakao_skill(request: Request):
    # ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ìš”ì²­ íŒŒì‹±
    req = await request.json()

    # ì‚¬ìš©ì ë°œí™”(query) ì¶”ì¶œ
    user_query = req['userRequest']['utterance']

    # LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
    answer = answer_with_rag(user_query)

    # ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    res = {
        "version": "2.0",
        "template": {
            "outputs": [
                {
                    "textCard": {
                        "title": f'ğŸ¤– {user_query} ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.',
                        "description": answer,
                        "buttons": [
                            {
                                "action": "webLink",
                                "label": "ğŸŒ ì§€ì—­ê³µì•½ ì‚´í´ë³´ê¸°",
                                "webLinkUrl": "https://www.naver.com/"
                            },
                            {
                                "action": "webLink",
                                "label": "ğŸ“š ì •ì±…ê³µì•½ì§‘ ì‚´í´ë³´ê¸°",
                                "webLinkUrl": "https://www.naver.com/"
                            }
                        ]
                    }
                }
            ]
        }
    }

    return res

# ì„œë²„ ì´ˆê¸°í™” ë° ì‹¤í–‰ì„ ìœ„í•œ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    # ëª¨ë¸ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    init_success = init_rag_system()

    if not init_success:
        print("ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        import sys
        sys.exit(1)
    else:
        print("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ! ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    uvicorn.run("chatbot-fastapi:app", host="0.0.0.0", port=5000)
