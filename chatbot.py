import os
import re
import json
import torch
import uvicorn
import asyncio
import httpx
from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

app = FastAPI()

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
pdf_paths = [
    "ì •ì±…ê³µì•½ì§‘.pdf",
    "ì§€ì—­ê³µì•½.pdf"
]

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ ë²¡í„° ìŠ¤í† ì–´ ì„ ì–¸
model = None
tokenizer = None
vectorstore = None

def init_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global model, tokenizer, vectorstore

    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1. PDF ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = []
    for pdf_path in pdf_paths:
        if os.path.exists(pdf_path):
            loader = PyPDFLoader(pdf_path)
            documents.extend(loader.load())
        else:
            print(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")

    if not documents:
        print("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    print(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # 3. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")

    for doc_id in list(vectorstore.docstore._dict.keys())[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥í•´ ë¡œê·¸ í¬ê¸° ì œí•œ
        print(f"ë¬¸ì„œ ID {doc_id}ì˜ ë‚´ìš© (ìƒ˜í”Œ):")
        print(vectorstore.docstore._dict[doc_id])
        print("-" * 50)

    # 5. EXAONE ëª¨ë¸ ë¡œë“œ
    model_name = "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct"
    print(f"{model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ì–‘ìí™” ì„¤ì •ì„ BitsAndBytesConfigë¡œ êµ¬ì„±
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    # ëª¨ë¸ ë¡œë“œ - ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("EXAONE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©
    optimize_performance()

    return True

def optimize_performance():
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©"""
    global model

    # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
        torch.cuda.set_device(torch.cuda.current_device())

    # ëª¨ë¸ ìµœì í™”
    if hasattr(model, 'config'):
        if hasattr(model.config, 'use_cache'):
            model.config.use_cache = True
        if hasattr(model.config, 'gradient_checkpointing'):
            model.config.gradient_checkpointing = False

    print("ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

def retrieve_context(query, k):
    """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    global vectorstore
    relevant_docs = vectorstore.similarity_search(query, k=k)

    context = "\n\n".join([doc.page_content for doc in relevant_docs])
    return context

def generate_answer(prompt):
    """í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    global model, tokenizer

    # ì…ë ¥ ì¸ì½”ë”© - attention_mask ëª…ì‹œì  ì„¤ì •
    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)

    # íš¨ìœ¨ì ì¸ ìƒì„± ì„¤ì •
    generation_config = {
        "max_new_tokens": 500,
        # "do_sample": True,
        # "temperature": temperature,
        "num_beams": 1,
        "pad_token_id": tokenizer.eos_token_id,
    }

    # í† í° ìƒì„±
    with torch.no_grad():
        output = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,  # attention_mask ëª…ì‹œì  ì „ë‹¬
            **generation_config
        )

    # ì‘ë‹µ ë””ì½”ë”©
    response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response.strip()

def post_process_answer(answer):
    """ì‘ë‹µì„ í›„ì²˜ë¦¬í•˜ì—¬ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì •ì œí•©ë‹ˆë‹¤."""
    # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ì œê±°
    answer = re.sub(r'\*\*(.*?)\*\*', r'\1', answer)
    
    # "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤" íŒ¨í„´ ì œê±°
    answer = re.sub(r'^(ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤|ê´€ë ¨ ê³µì•½ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤|ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤|ë‹¤ìŒê³¼ ê°™ì€ ê³µì•½ì´ ìˆìŠµë‹ˆë‹¤|ë‹¤ìŒì„ ì°¸ê³ í•˜ì„¸ìš”)[\.:]?\s*', '', answer)
    
    # "~ì…ë‹ˆë‹¤"ë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´ ì œê±°
    answer = re.sub(r'^[^\.]*ì…ë‹ˆë‹¤[\.:]?\s*', '', answer)
    
    # ëª¨ë“  ì½œë¡  ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€
    answer = re.sub(r':\s*', ':\n', answer)
    
    # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ì •ë¦¬
    answer = re.sub(r'(\d+)\.\s+', r'\n\1. ', answer)
    
    # ë¶ˆí•„ìš”í•œ ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
    answer = re.sub(r'(<br>){3,}', '<br>', answer)
    
    # ì²« ë²ˆì§¸ ì¤„ì´ ë¹„ì–´ ìˆìœ¼ë©´ ì œê±°
    answer = re.sub(r'^(<br>)', '', answer)

    # í•­ëª© ì•ì˜ ê³¼ë„í•œ ê³µë°± ì •ë¦¬ (- ê¸°í˜¸ ì•ì˜ ê³µë°± ì—†ì•°)
    answer = re.sub(r'^\s+- ', r'- ', answer, flags=re.MULTILINE)
    
    # ì‘ë‹µì´ ë¹„ì–´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not answer.strip():
        answer = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    return answer.strip()

async def process_llm_and_callback(user_query: str, callback_url: str):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ LLM ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê³ , ì™„ë£Œë˜ë©´ callbackUrlë¡œ ì‘ë‹µì„ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    try:
        # 1. RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° í”„ë¡¬í”„íŠ¸ ìƒì„± (ì´ ë¶€ë¶„ì€ ë¹„êµì  ë¹ ë¥¼ ìˆ˜ ìˆìŒ)
        context = retrieve_context(user_query, k=5)

        prompt = f"""ì œê³µí•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
                  ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.

                  ### ì‘ë‹µ í˜•ì‹ ###
                  ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì€ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
                  1. ì§ˆë¬¸ ì£¼ì œì™€ ê´€ë ¨ëœ ê³µì•½ ë˜ëŠ” ì •ì±…ì„ í•­ëª©ë³„ë¡œ ë‚˜ëˆ  ì‘ì„±í•©ë‹ˆë‹¤.
                  2. ëª¨ë“  ìš”ì  ì•ì—ëŠ” ë²ˆí˜¸ë‚˜ ê¸°í˜¸(ì˜ˆ: 1. 2. 3. ë˜ëŠ” - - -)ë¥¼ ë¶™ì—¬ í•­ëª©ë³„ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
                  3. ê° í•­ëª©ì€ ë‹¤ìŒ ì¤„ì— ì‘ì„±í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
                  4. "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤", "~ ì…ë‹ˆë‹¤", "ë‹¤ìŒì„ ì°¸ê³ í•˜ì„¸ìš”" ë“±ì˜ í‘œí˜„ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ë§ˆì„¸ìš”.
                  5. ê° í•­ëª©ì€ ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

                  ### ì°¸ê³  ì •ë³´: ###
                  {context}

                  ### ì‚¬ìš©ì ì§ˆë¬¸ ###
                  {user_query}ì™€ ê´€ë ¨ëœ ê³µì•½ë§Œ ë¬¸ì„œì— ìˆëŠ” ê·¸ëŒ€ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

                  ### ë‹µë³€ ###"""

        # 2. LLM ì‘ë‹µ ìƒì„±
        print(f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘: LLM ì‘ë‹µ ìƒì„± ì¤‘... (ì¿¼ë¦¬: {user_query})")
        raw_answer = await asyncio.to_thread(generate_answer, prompt) # ë™ê¸° í•¨ìˆ˜ë¥¼ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸° ì‹¤í–‰
        print(f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì™„ë£Œ: LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ. í›„ì²˜ë¦¬ ì‹œì‘.")

        # 3. ì‘ë‹µ í›„ì²˜ë¦¬
        final_answer = post_process_answer(raw_answer)

        print(final_answer)

        print(f"í›„ì²˜ë¦¬ ì™„ë£Œ. ì½œë°± URLë¡œ ì‘ë‹µ ì „ì†¡ ì¤€ë¹„: {callback_url}")

        # 4. callbackUrlë¡œ ìµœì¢… ì‘ë‹µ ì „ì†¡
        # ì¹´ì¹´ì˜¤í†¡ ì½œë°± ì‘ë‹µ í˜•ì‹ì— ë§ì¶° JSON ë°ì´í„° êµ¬ì„±
        callback_res = {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": f'ğŸŒ³ {user_query} ê´€ë ¨ ë‹µë³€ì…ë‹ˆë‹¤.\n\n{final_answer}'
                        }
                    }
                ]
            }
            }

        async with httpx.AsyncClient() as client:
            response = await client.post(callback_url, json=callback_res, timeout=60.0)
            print(f"ì½œë°± URL ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
            print("ì½œë°± URLë¡œ ìµœì¢… ì‘ë‹µ ì „ì†¡ ì„±ê³µ.")

    except Exception as e:
        print(f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        try:
             error_res = {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {
                            "simpleText": {
                                "text": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                            }
                        }
                    ]
                }
             }
             async with httpx.AsyncClient() as client:
                 await client.post(callback_url, json=error_res, timeout=30.0)
                 print("ì˜¤ë¥˜ ì•ˆë‚´ ì½œë°± ì „ì†¡ ì„±ê³µ.")
        except Exception as e_callback:
            print(f"ì˜¤ë¥˜ ì•ˆë‚´ ì½œë°± ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_callback}")


# ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •
@app.post('/api/chat')
async def kakao_skill(request: Request, background_tasks: BackgroundTasks): # BackgroundTasks íŒŒë¼ë¯¸í„° ì¶”ê°€
    print("ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ìš”ì²­ ìˆ˜ì‹ ")
    req = await request.json()

    # ì‚¬ìš©ì ë°œí™”(query) ì¶”ì¶œ
    user_query = req['userRequest']['utterance']

    # callbackUrl ì¶”ì¶œ
    callback_url = req['userRequest'].get('callbackUrl')

    print(f"ì‚¬ìš©ì ì¿¼ë¦¬: {user_query}")
    print(f"Callback URL: {callback_url}")

    background_tasks.add_task(process_llm_and_callback, user_query, callback_url)

    print("ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë“±ë¡ ì™„ë£Œ. ì¦‰ì‹œ ì‘ë‹µ ì „ì†¡.")

    # ì¹´ì¹´ì˜¤í†¡ ì„œë²„ì— 5ì´ˆ ì´ë‚´ë¡œ ì¤‘ê°„ ì‘ë‹µì„ ë°˜í™˜
    immediate_res = {
        "version" : "2.0",
        "useCallback" : True
    }

    return JSONResponse(content=immediate_res)

@app.on_event("startup")
async def startup_event():
    # ëª¨ë¸ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    init_success = init_rag_system()

    if not init_success:
        print("ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        import sys
        sys.exit(1)
    else:
        print("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ! ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    uvicorn.run("chatbot:app", host="0.0.0.0", port=5000)