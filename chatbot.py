import os
import re
import json
import torch
import uvicorn
import asyncio
import httpx
from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, BitsAndBytesConfig, pipeline

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

app = FastAPI()

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
pdf_paths = [
    "ì •ì±…ê³µì•½ì§‘.pdf",
    "ì§€ì—­ê³µì•½.pdf"
]

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ ë²¡í„° ìŠ¤í† ì–´ ì„ ì–¸
model = None
tokenizer = None
vectorstore = None
filter_model = None
filter_tokenizer = None

def init_filter_model():
    """ë¶€ì ì ˆí•œ ì½˜í…ì¸  í•„í„°ë§ ëª¨ë¸ ì´ˆê¸°í™”"""
    global filter_model, filter_tokenizer
    
    print("í•„í„°ë§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    filter_model_name = "beomi/kcbert-base"
    
    try:
        filter_tokenizer = AutoTokenizer.from_pretrained(
            filter_model_name
        )
        
        filter_model = AutoModelForSequenceClassification.from_pretrained(
            filter_model_name
        ).to("cuda" if torch.cuda.is_available() else "cpu")
        
        print("í•„í„°ë§ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
        return True
    except Exception as e:
        print(f"í•„í„°ë§ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        filter_model = None
        filter_tokenizer = None
        return True  # ëª¨ë¸ ì—†ì´ë„ ì„œë²„ëŠ” ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ True ë°˜í™˜

def is_inappropriate(text: str) -> bool:
    """í…ìŠ¤íŠ¸ê°€ ë¶€ì ì ˆí•œì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ (ìš•ì„¤, í˜ì˜¤í‘œí˜„ ë“± í¬í•¨ ì—¬ë¶€ íƒì§€)"""
    global filter_model, filter_tokenizer
    
    # ë¸”ë™ë¦¬ìŠ¤íŠ¸
    inappropriate_keywords = [
        "ì‹œë°œ", "ì”¨ë°œ", "ë³‘ì‹ ", "ê°œìƒˆë¼", "ì¢†", "ìƒˆë¼", "ì§€ë„", "ì”¹", 
        "NSFW", "ì•¼ë™", "í¬ë¥´ë…¸", "ì„±ì¸ë¬¼"
    ]
    
    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
    policy_keywords = [
        "ë³µì§€", "êµìœ¡", "ê²½ì œ", "í™˜ê²½", "ë¬¸í™”", "ì•ˆì „", "êµ­ë°©", "ì™¸êµ", 
        "ì¼ìë¦¬", "ì²­ë…„", "ë…¸ì¸", "ì•„ë™", "ì—¬ì„±", "ì¥ì• ì¸", "ì†Œë“", "ì„¸ê¸ˆ", 
        "ì˜ë£Œ", "ì£¼íƒ", "êµí†µ", "ë°˜ë ¤ë™ë¬¼", "ë†ì—…", "ì–´ì—…", "ì‚°ì—…"
    ]
    
    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í™•ì¸
    for keyword in policy_keywords:
        if keyword in text:
            print(f"ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œ '{keyword}' ê°ì§€ë¨ - í•„í„°ë§ í†µê³¼")
            return False  # ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì ì ˆí•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
    
    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ í™•ì¸
    for keyword in inappropriate_keywords:
        if keyword in text:
            print(f"ë¶€ì ì ˆí•œ í‚¤ì›Œë“œ '{keyword}' ê°ì§€ë¨")
            return True  # ë¶€ì ì ˆí•œ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¶€ì ì ˆí•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
    
    # ëª¨ë¸ ê¸°ë°˜ í•„í„°ë§ (ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš°ì—ë§Œ)
    if filter_model and filter_tokenizer:
        try:
            inputs = filter_tokenizer(text, return_tensors="pt", truncation=True, max_length=128).to(filter_model.device)
            
            with torch.no_grad():
                outputs = filter_model(**inputs)
            
            # ì ìˆ˜ ê³„ì‚° - ê¸°ì¡´ ëª¨ë¸ì€ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ì´ë¯€ë¡œ, ë‘ ë²ˆì§¸ í´ë˜ìŠ¤(index=1)ì˜ í™•ë¥ ì„ ê³„ì‚°
            # ì´ ëª¨ë¸ì´ ë¶€ì ì ˆí•œ ë‚´ìš© íƒì§€ì— ì™„ë²½í•˜ê²Œ ë§ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‚®ì€ ì„ê³„ê°’ ì ìš©
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            inappropriate_score = probabilities[0][1].item()  # ë‘ ë²ˆì§¸ í´ë˜ìŠ¤ì˜ í™•ë¥ 
            
            print(f"í…ìŠ¤íŠ¸ ë¶€ì ì ˆ ì ìˆ˜: {inappropriate_score:.4f}")
            
            # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’ ì ìš© (0.3) - ë¯¼ê°í•œ í•„í„°ë§ í”¼í•˜ê¸°
            return inappropriate_score > 0.3
        except Exception as e:
            print(f"ëª¨ë¸ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False  # ì˜¤ë¥˜ ë°œìƒ ì‹œ í†µê³¼ í—ˆìš©
    
    return False

def init_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global model, tokenizer, vectorstore

    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1. PDF ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = []
    for pdf_path in pdf_paths:
        if os.path.exists(pdf_path):
            loader = PyPDFLoader(pdf_path)
            documents.extend(loader.load())
        else:
            print(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")

    if not documents:
        print("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    print(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # 3. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")

    for doc_id in list(vectorstore.docstore._dict.keys())[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥í•´ ë¡œê·¸ í¬ê¸° ì œí•œ
        print(f"ë¬¸ì„œ ID {doc_id}ì˜ ë‚´ìš© (ìƒ˜í”Œ):")
        print(vectorstore.docstore._dict[doc_id])
        print("-" * 50)

    # 5. EXAONE ëª¨ë¸ ë¡œë“œ
    model_name = "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct"
    print(f"{model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ì–‘ìí™” ì„¤ì •ì„ BitsAndBytesConfigë¡œ êµ¬ì„±
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    # ëª¨ë¸ ë¡œë“œ - ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("EXAONE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©
    optimize_performance()

    return True

def optimize_performance():
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©"""
    global model

    # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
        torch.cuda.set_device(torch.cuda.current_device())

    # ëª¨ë¸ ìµœì í™”
    if hasattr(model, 'config'):
        if hasattr(model.config, 'use_cache'):
            model.config.use_cache = True
        if hasattr(model.config, 'gradient_checkpointing'):
            model.config.gradient_checkpointing = False

    print("ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

def retrieve_context(query, k):
    """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    global vectorstore
    relevant_docs = vectorstore.similarity_search(query, k=k)

    context = "\n\n".join([doc.page_content for doc in relevant_docs])
    return context

def generate_answer(prompt):
    """í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    global model, tokenizer

    # ì…ë ¥ ì¸ì½”ë”© - attention_mask ëª…ì‹œì  ì„¤ì •
    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)

    # íš¨ìœ¨ì ì¸ ìƒì„± ì„¤ì •
    generation_config = {
        "max_new_tokens": 500,
        # "do_sample": True,
        # "temperature": temperature,
        "num_beams": 1,
        "pad_token_id": tokenizer.eos_token_id,
    }

    # í† í° ìƒì„±
    with torch.no_grad():
        output = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,  # attention_mask ëª…ì‹œì  ì „ë‹¬
            **generation_config
        )

    # ì‘ë‹µ ë””ì½”ë”©
    response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return response.strip()

def post_process_answer(answer):
    """ì‘ë‹µì„ í›„ì²˜ë¦¬í•˜ì—¬ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì •ì œí•©ë‹ˆë‹¤."""
    # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ì œê±°
    answer = re.sub(r'\*\*(.*?)\*\*', r'\1', answer)
    
    # "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤" íŒ¨í„´ ì œê±°
    answer = re.sub(r'^(ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤|ê´€ë ¨ ê³µì•½ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤|ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤|ë‹¤ìŒê³¼ ê°™ì€ ê³µì•½ì´ ìˆìŠµë‹ˆë‹¤|ë‹¤ìŒì„ ì°¸ê³ í•˜ì„¸ìš”)[\.:]?\s*', '', answer)
    
    # "~ì…ë‹ˆë‹¤"ë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´ ì œê±°
    answer = re.sub(r'^[^\.]*ì…ë‹ˆë‹¤[\.:]?\s*', '', answer)
    
    # ëª¨ë“  ì½œë¡  ë’¤ì— ì¤„ë°”ê¿ˆ ì¶”ê°€
    answer = re.sub(r':\s*', ':\n', answer)
    
    # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ì •ë¦¬
    answer = re.sub(r'(\d+)\.\s+', r'\n\1. ', answer)
    
    # ë¶ˆí•„ìš”í•œ ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
    answer = re.sub(r'(<br>){3,}', '<br>', answer)
    
    # ì²« ë²ˆì§¸ ì¤„ì´ ë¹„ì–´ ìˆìœ¼ë©´ ì œê±°
    answer = re.sub(r'^(<br>)', '', answer)

    # í•­ëª© ì•ì˜ ê³¼ë„í•œ ê³µë°± ì •ë¦¬ (- ê¸°í˜¸ ì•ì˜ ê³µë°± ì—†ì•°)
    answer = re.sub(r'^\s+- ', r'- ', answer, flags=re.MULTILINE)
    
    # ì‘ë‹µì´ ë¹„ì–´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
    if not answer.strip():
        answer = "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    return answer.strip()

async def process_llm_and_callback(user_query: str, callback_url: str):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ LLM ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê³ , ì™„ë£Œë˜ë©´ callbackUrlë¡œ ì‘ë‹µì„ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    try:
        # 1. RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° í”„ë¡¬í”„íŠ¸ ìƒì„± (ì´ ë¶€ë¶„ì€ ë¹„êµì  ë¹ ë¥¼ ìˆ˜ ìˆìŒ)
        context = retrieve_context(user_query, k=5)

        prompt = f"""ì œê³µí•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
                  ë¬¸ì„œ ë‚´ìš©ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.

                  ### ì‘ë‹µ í˜•ì‹ ###
                  ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì€ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
                  1. ì§ˆë¬¸ ì£¼ì œì™€ ê´€ë ¨ëœ ê³µì•½ ë˜ëŠ” ì •ì±…ì„ í•­ëª©ë³„ë¡œ ë‚˜ëˆ  ì‘ì„±í•©ë‹ˆë‹¤.
                  2. ëª¨ë“  ìš”ì  ì•ì—ëŠ” ë²ˆí˜¸ë‚˜ ê¸°í˜¸(ì˜ˆ: 1. 2. 3. ë˜ëŠ” - - -)ë¥¼ ë¶™ì—¬ í•­ëª©ë³„ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
                  3. ê° í•­ëª©ì€ ë‹¤ìŒ ì¤„ì— ì‘ì„±í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
                  4. "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤", "~ ì…ë‹ˆë‹¤", "ë‹¤ìŒì„ ì°¸ê³ í•˜ì„¸ìš”" ë“±ì˜ í‘œí˜„ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ë§ˆì„¸ìš”.
                  5. ê° í•­ëª©ì€ ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

                  ### ì°¸ê³  ì •ë³´: ###
                  {context}

                  ### ì‚¬ìš©ì ì§ˆë¬¸ ###
                  {user_query}ì™€ ê´€ë ¨ëœ ê³µì•½ë§Œ ë¬¸ì„œì— ìˆëŠ” ê·¸ëŒ€ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

                  ### ë‹µë³€ ###"""

        # 2. LLM ì‘ë‹µ ìƒì„±
        print(f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘: LLM ì‘ë‹µ ìƒì„± ì¤‘... (ì¿¼ë¦¬: {user_query})")
        raw_answer = await asyncio.to_thread(generate_answer, prompt) # ë™ê¸° í•¨ìˆ˜ë¥¼ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸° ì‹¤í–‰
        print(f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì™„ë£Œ: LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ. í›„ì²˜ë¦¬ ì‹œì‘.")

        # 3. ì‘ë‹µ í›„ì²˜ë¦¬
        final_answer = post_process_answer(raw_answer)

        print(final_answer)

        print(f"í›„ì²˜ë¦¬ ì™„ë£Œ. ì½œë°± URLë¡œ ì‘ë‹µ ì „ì†¡ ì¤€ë¹„: {callback_url}")

        # 4. callbackUrlë¡œ ìµœì¢… ì‘ë‹µ ì „ì†¡
        # ì¹´ì¹´ì˜¤í†¡ ì½œë°± ì‘ë‹µ í˜•ì‹ì— ë§ì¶° JSON ë°ì´í„° êµ¬ì„±
        callback_res = {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": f'ğŸŒ³ {user_query} ê´€ë ¨ ë‹µë³€ì…ë‹ˆë‹¤.\n\n{final_answer}'
                        }
                    }
                ]
            }
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(callback_url, json=callback_res, timeout=60.0)
            print(f"ì½œë°± URL ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
            print("ì½œë°± URLë¡œ ìµœì¢… ì‘ë‹µ ì „ì†¡ ì„±ê³µ.")

    except Exception as e:
        print(f"ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        try:
             error_res = {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {
                            "simpleText": {
                                "text": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                            }
                        }
                    ]
                }
             }
             async with httpx.AsyncClient() as client:
                 await client.post(callback_url, json=error_res, timeout=30.0)
                 print("ì˜¤ë¥˜ ì•ˆë‚´ ì½œë°± ì „ì†¡ ì„±ê³µ.")
        except Exception as e_callback:
            print(f"ì˜¤ë¥˜ ì•ˆë‚´ ì½œë°± ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_callback}")


# ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •
@app.post('/api/chat')
async def kakao_skill(request: Request, background_tasks: BackgroundTasks):
    print("ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ìš”ì²­ ìˆ˜ì‹ ")
    req = await request.json()

    # ì‚¬ìš©ì ë°œí™”(query) ì¶”ì¶œ
    user_query = req['userRequest']['utterance']

    # callbackUrl ì¶”ì¶œ
    callback_url = req['userRequest'].get('callbackUrl')

    print(f"ì‚¬ìš©ì ì¿¼ë¦¬: {user_query}")
    print(f"Callback URL: {callback_url}")
    
    # í•„í„°ë§ ìˆ˜í–‰
    if is_inappropriate(user_query):
        print(f"ë¶€ì ì ˆí•œ ì½˜í…ì¸  íƒì§€: '{user_query}'")
        
        # ì¦‰ì‹œ ë¶€ì ì ˆ ì‘ë‹µ ë°˜í™˜
        return JSONResponse(content={
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶€ì ì ˆí•œ ì–¸ì–´ë‚˜ ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        }
                    }
                ]
            }
        })

    # ì •ìƒ ë°œí™”ë©´ ê¸°ì¡´ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìˆ˜í–‰
    background_tasks.add_task(process_llm_and_callback, user_query, callback_url)

    print("ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë“±ë¡ ì™„ë£Œ. ì¦‰ì‹œ ì‘ë‹µ ì „ì†¡.")

    # ì¹´ì¹´ì˜¤í†¡ ì„œë²„ì— 5ì´ˆ ì´ë‚´ë¡œ ì¤‘ê°„ ì‘ë‹µì„ ë°˜í™˜
    immediate_res = {
        "version": "2.0",
        "useCallback": True
    }

    return JSONResponse(content=immediate_res)

@app.on_event("startup")
async def startup_event():
    # í•„í„°ë§ ëª¨ë¸ ì´ˆê¸°í™”
    filter_init_success = init_filter_model()
    if not filter_init_success:
        print("í•„í„°ë§ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨!")
        import sys
        sys.exit(1)
    
    # ëª¨ë¸ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_init_success = init_rag_system()
    if not rag_init_success:
        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        import sys
        sys.exit(1)
    
    print("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ! ì¹´ì¹´ì˜¤í†¡ ìŠ¤í‚¬ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    uvicorn.run("chatbot:app", host="0.0.0.0", port=5000)